---
title: "Random forest model response targets"
output: html_document
---

```{r setup, include = FALSE}
library(targets)
library(tidymodels)
```

## Recipe for the model

```{targets recRFResponse}
tar_target(recRFResponse, recipe(sbp_post ~ sbp_pre +
    nitro_pre +
    nitro_diff_mcg +
    total_nitro +
    n_nitro +
    nitro_time +
    time_since_nitro +
    sbp_pre_mean_60 +
    sbp_pre_sd_60 +
    pain_score +
    lag_nitro_diff +
    lag_sbp_diff,
    data = trainingResponse
  )|>
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~lag_nitro_diff:lag_sbp_diff) #This is the interaction term

  )
```

## Model specification

```{targets specRFResponse}
tar_target(
    specRFResponse,
    rand_forest(
      trees = tune(),
      min_n = tune(),
      mtry = tune(),
    ) |>
      set_engine("randomForest")|>
      set_mode("regression")
  )
```

Model workflow

```{targets workflowRFResponse}
 tar_target(
    workflowRFResponse,
    workflow()|>
    add_recipe(recRFResponse)|>
    add_model(specRFResponse)
  )
```

Grid search ranges for tuning model parameters

```{targets gridRFResponse}
tar_target(gridRFResponse,
  grid_max_entropy(
    min_n(c(20L, 40L)),
    mtry(c(4L, 6L)),
    trees(c(500, 5000))
    )
  )
```

The tune_race_anova function can be used to speed up the process of finding suitable parameters for the model. Results are returned from 5-fold cross validation. 

```{targets tuningRaceRFResponse}
tar_target(tuningRaceRFResponse,
      tune_race_anova(
        workflowRFResponse,
        foldsFive,
        grid = gridRFResponse,
        metrics = mset,
        control = control_race(verbose_elim = TRUE)
      )
  )
```

A simple tuning grid is useful when interested in tuning just a couple of parameters.

```{targets tuningGridBoost}
# tar_target(tuningGridBoost,
#       tune_grid(
#        workflowBoost,
#         foldsFive,
#         grid = crossing(
#           min_n = seq(30,40, 5)
#     ),
#         metrics = mset,
#         control = control
#       )
#   )
```

Once parameters are chosen, we can check how they performed with 5-fold cross-validation.

```{targets resampleBoost}
# tar_target(resampleBoost,
#   fit_resamples(
#     specBoost,
#     recBoost,
#     resamples = foldsFive,
#     control = controlResamples
#          )
# )
```


## Final model evaluation

The best model from cross-validation is chosen to update the final workflow. The last_fit function trains and then tests with the testing data. 

