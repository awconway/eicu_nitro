---
title: "Lasso model evaluation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, messages = FALSE)
```

## Check performance over a range of values for the penalty parameter

We see that the lasso model performed better with lower amounts of regularization.

```{r}
targets::tar_read(tuningLasso) |>
autoplot()
```

## Show the best models for the grid search

```{r}
tune::show_best(targets::tar_read(tuningLasso))|>
gt::gt()
```

## Assess varibale importance

```{r}
best <- select_best(targets::tar_read(tuningLasso))
lasso_best <- finalize_workflow(
    targets::tar_read(workflowLasso), best
)

lasso_best |>
fit(targets::tar_read(training)) |>
extract_fit_parsnip() |>
vip::vi(lambda = best$penalty) |>
mutate(
    Importance = abs(Importance),
    Variable = forcats::fct_reorder(Variable, Importance)
) |>
ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(y = NULL)
```

## Final model evaluation

Only run when predictor set and parameters for the model are selected.

```{r}
# targets::tar_read(lassoFinal)|>collect_metrics()
```
